{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Sheet 2\n",
    "\n",
    "In Tasks (a)-(c), consider a simplified case of $n=1$ and $h_{\\theta}(x) = \\theta x$. Assume quadratic pointwise loss, $\\ell(y,\\hat y) = (y-\\hat y)^2.$\n",
    "Assume that data are samples of a random variable pair $(X,Y) \\sim \\mathbb{P}$.\n",
    "\n",
    "## Task (a) (Warm-up)\n",
    "\n",
    "- Find the expected risk $\\mathbb{E}[\\ell(h_{\\theta}(X),Y)]$ in terms of $\\theta$ and suitable expectations.\n",
    "- Find the best parameter $\\theta^{best} = \\arg\\min_{\\theta \\in \\mathbb{R}} \\mathbb{E}[\\ell(h_{\\theta}(X),Y)].$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task (b)\n",
    "- Assume $X$ is uniformly distributed on $[-1,1]$, $X \\sim \\mathcal{U}(-1,1)$, and consider two options of generating data:\n",
    "  - $Y = X$, and \n",
    "  - $Y = X - X^3$.\n",
    "  \n",
    "For each of those options for $Y$, compute $\\theta^{best}$ and the corresponding expected risk $\\mathbb{E}[\\ell(h_{\\theta^{best}}(X),Y)]$. What can you say about these $\\theta^{best}$ and the accuracy of the prediction rule $h_{\\theta^{best}}$ in each case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task (c) \n",
    "Assume $X \\sim \\mathcal{U}(-1,1)$ as before, but the data $Y = X + \\xi$ is **noisy**, that is, a function of $X$ perturbed by another random variable $\\xi$ which is **independent of** $X$. Assume $\\mathbb{E}[\\xi] = 0$ and $\\mathrm{Var}[\\xi] < \\infty$, where $\\mathrm{Var}[\\xi]$ is the variance.\n",
    "- Find $\\theta^{best} = \\arg\\min_{\\theta \\in \\mathbb{R}} \\mathbb{E}[\\ell(h_{\\theta}(X),Y)]$ under these new assumptions. Compare it with $\\theta^{best}$ from Task (b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: simulating the \"true\" distribution\n",
    "\n",
    "For testing the convergence of the empirical risk we cannot use the temperature data (or any actual data in fact), since it's finite and we cannot let $|D| \\rightarrow \\infty$.\n",
    "\n",
    "To select a _class_ of models, or to test numerical algorithms, one often designs a **synthetic** distribution with known properties. One can then sample as many realisations from this distribution as desired.\n",
    "\n",
    "- **Write** a Python **function** `TrueSample()` which returns a pair x,y which is a random sample from the following distribution:\n",
    "   - $X \\sim \\mathcal{U}(-1,1)$ is a random value uniformly distributed between -1 and 1;\n",
    "   - for a given $X$, compute the label as $Y = X - X^3 + \\frac{1}{10} \\xi$, where $\\xi \\sim \\mathcal{N}(0,1)$ is a random variable with the standard normal distribution.   \n",
    "   \n",
    "   _Hint: read up on [numpy.random](https://numpy.org/doc/1.16/reference/routines.random.html) module_\n",
    " \n",
    "- **Compute** 30 samples of x and y from `TrueSample()` and store them in _numpy_ arrays X and Y, respectively.\n",
    "- **Plot** array Y vs array X using `plot()` from _matplotlib_'s _pyplot_ module. Make sure you plot **only points** (x,y) (not lines connecting them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: convergence of the parameter\n",
    "- **Copy** over functions `features` and `optimise_loss` from Problem Sheet 1 (your implementation or my model solution)\n",
    "- Take $n=1$ and **compute** $\\boldsymbol\\theta^* \\in \\mathbb{R}^2$ using `optimise_loss` applied to the data X and Y prepared in Task 1.\n",
    "- **Vary** the number of samples in Task 1 in a range 30, 100, 300, 1000, 3000, and for each corresponding realisation of X and Y compute the corresponding $\\boldsymbol\\theta^*$.\n",
    "- **Compare** $\\theta^*_1$ with $\\theta^{best}$ found in Task (b) as the number of samples increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 (Warm-up): data splitting\n",
    "\n",
    "- **Write a Python function** `split_data(X,Y,K,k)` that takes as input _numpy_ arrays X and Y as defined previously, a positive integer number of chunks K the data should be split into, and an index $0\\le$ k $<$ K. The function should split X and Y into K subsets of equal size (you can assume that K divides the size of X and Y) and **return** 4 outputs: Xtrain, Ytrain, Xtest, Ytest, where Xtest and Ytest are the $k$-th subsets of X and Y, respectively, and Xtrain and Ytrain contain the rest of X and Y, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
