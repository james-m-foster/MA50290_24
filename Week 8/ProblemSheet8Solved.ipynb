{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe65ff25",
   "metadata": {},
   "source": [
    "# Problem Sheet 8\n",
    "\n",
    "## Convexity and Stochastic Gradient Descent (SGD) method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b31cdf3",
   "metadata": {},
   "source": [
    "## Task (a)\n",
    "Consider $\\boldsymbol\\theta=(\\theta_1,\\theta_2) \\in \\mathbb{R}^2$ and the function\n",
    "$$\n",
    "L(\\boldsymbol\\theta) = \\theta_1^2 + (\\theta_1 - \\theta_2)^2.\n",
    "$$\n",
    "- Prove that $L(\\boldsymbol\\theta)$ is convex on $\\mathbb{R}^2$.\n",
    "\n",
    "_Hint: use Lemma 4.24 and that_ $2ab \\le a^2+b^2$ _for any_ $a,b \\in \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a2a2f8",
   "metadata": {},
   "source": [
    "## Solution\n",
    "$$\n",
    "L(\\boldsymbol\\theta) = 2\\theta_1^2 - 2 \\theta_1 \\theta_2 + \\theta_2^2,\n",
    "$$\n",
    "hence\n",
    "$$\n",
    "\\nabla L(\\boldsymbol\\theta) = \\begin{bmatrix}4 \\theta_1 - 2 \\theta_2 \\\\ -2 \\theta_1 + 2 \\theta_2\\end{bmatrix}.\n",
    "$$\n",
    "Using Lemma 4.24, to check convexity we can check nonnegativity of the following inner product:\n",
    "\\begin{align*}\n",
    "\\langle \\nabla L(\\boldsymbol\\eta) - \\nabla L(\\boldsymbol\\theta), \\boldsymbol\\eta - \\boldsymbol\\theta \\rangle & = \\left\\langle\\begin{bmatrix}4 (\\eta_1-\\theta_1) - 2 (\\eta_2-\\theta_2) \\\\ -2 (\\eta_1-\\theta_1) + 2 (\\eta_2-\\theta_2)\\end{bmatrix}, \\begin{bmatrix}\\eta_1 - \\theta_1 \\\\ \\eta_2 - \\theta_2\\end{bmatrix} \\right\\rangle \\\\\n",
    "& = 4 (\\eta_1-\\theta_1)^2 - 4 \\underbrace{(\\eta_1 - \\theta_1)}_a \\underbrace{(\\eta_2 - \\theta_2)}_b + 2 (\\eta_2 - \\theta_2)^2 \\\\\n",
    "& \\ge 4 (\\eta_1-\\theta_1)^2 - 2 (\\eta_1-\\theta_1)^2 - 2 (\\eta_2-\\theta_2)^2 + 2 (\\eta_2-\\theta_2)^2 \\\\\n",
    "& = 2 (\\eta_1 - \\theta_1)^2 \\ge 0.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508670a6",
   "metadata": {},
   "source": [
    "## Task (b) (Warm-up)\n",
    "Consider the same function as in Task (a).\n",
    "- Find $m \\in \\mathbb{N}$, $\\mathbf{x}_i \\in \\mathbb{R}^2$ and $y_i \\in \\mathbb{R}$, $i=1,\\ldots,m$, such that $L(\\boldsymbol\\theta)$ is equal to the loss $L(\\boldsymbol\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}(\\langle \\boldsymbol\\theta, \\mathbf{x}_i \\rangle - y_i)^2$ of the linear regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f032d7",
   "metadata": {},
   "source": [
    "## Solution:\n",
    "We can rewrite\n",
    "\\begin{align*}\n",
    "L(\\boldsymbol\\theta) & = \\langle \\boldsymbol\\theta, [1,0] \\rangle^2 + \\langle \\boldsymbol\\theta, [1,-1] \\rangle^2 \\\\\n",
    "& = \\frac{1}{2}\\left(\\langle \\boldsymbol\\theta, [\\sqrt{2},0] \\rangle^2 + \\langle \\boldsymbol\\theta, [\\sqrt{2},-\\sqrt{2}] \\rangle^2 \\right)\n",
    "\\end{align*}\n",
    "\n",
    "So $m=2$, $y_1=y_2=0$, and \n",
    "$$\n",
    "\\mathbf{x}_1 = \\begin{bmatrix}\\sqrt{2} \\\\ 0\\end{bmatrix}, \\quad \\mathbf{x}_2 = \\begin{bmatrix}\\sqrt{2} \\\\ -\\sqrt{2}\\end{bmatrix} \\quad \\Rightarrow \\quad X = \\sqrt{2} \\begin{bmatrix}1 & 1 \\\\ 0 & -1\\end{bmatrix}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095de544",
   "metadata": {},
   "source": [
    "## Task (c)\n",
    "Consider the same function as in Task (a) and (b).\n",
    "- Prove that $L(\\boldsymbol\\theta)$ is $\\lambda$-strongly convex on $\\mathbb{R}^2$ and suggest a value of $\\lambda>0$.\n",
    " \n",
    "_Hint: use Task (b) and Theorem 4.32._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b553a9",
   "metadata": {},
   "source": [
    "## Solution:\n",
    "Firstly, we assemble\n",
    "$$\n",
    "A = \\frac{2}{m} XX^\\top = 2 \\begin{bmatrix}1 & 1 \\\\ 0 & -1\\end{bmatrix} \\begin{bmatrix}1 & 0 \\\\ 1 & -1\\end{bmatrix} = \\begin{bmatrix}4 & -2 \\\\ -2 & 2 \\end{bmatrix}.\n",
    "$$\n",
    "By Theorem 4.32, $\\lambda = \\lambda_{\\min}(A) = 3 - \\sqrt{5} \\approx 0.76$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b48ce68",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c6c269",
   "metadata": {},
   "source": [
    "## Task 1: SGD for polynomial regression\n",
    "\n",
    "- Write a Python function `sgd_poly(V, Y, theta0, t0, iters=10000)` which implements the Stochastic Gradient Descent method for the polynomial regression problem considered in Problem Sheet 7, taking \n",
    "$$\n",
    "\\mathbf{v}_k = \\nabla_{\\boldsymbol\\theta} \\tilde \\ell_{x_i,y_i}(\\boldsymbol\\theta_k) = 2 \\mathbf{v}_{i}^\\top (\\mathbf{v}_{i} \\boldsymbol\\theta - y_i),\n",
    "$$\n",
    "where $i=0,\\ldots,m-1$ is sampled uniformly at random in each iteration, and $V \\in \\mathbb{R}^{m \\times (d+1)}$ is as previously the Vandermonde matrix, and $\\mathbf{v}_i \\in \\mathbb{R}^{n+1}$ is the $i$-th row of $V$. Once again, the code to compute $V$ and `Y`$=[y_1,\\ldots,y_m]$ is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a253442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt    \n",
    "\n",
    "def TrueSample():\n",
    "    x = np.random.uniform(-1,1)\n",
    "    y = x - x**3 + 0.1*np.random.randn()\n",
    "    return x,y\n",
    "\n",
    "def features(x,n):\n",
    "    powers = np.arange(n+1)               # [0,1,2,...,n]\n",
    "    powers = np.reshape(powers, (1, -1))  # Make it explicitly a row vector\n",
    "    x = np.reshape(x, (-1, 1))            # Make it explicitly a column vector\n",
    "    return x**powers                      # Python automatically broadcasts the vectors to each others' shapes \n",
    "                                          # and takes the power between the resulting matrices elementwise\n",
    "\n",
    "Nsamples = 30\n",
    "Y = np.zeros(Nsamples)\n",
    "X = np.zeros(Nsamples)\n",
    "for i in range(Y.size):\n",
    "    X[i],Y[i] = TrueSample()\n",
    "\n",
    "n = 3\n",
    "V = features(X,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe44c33",
   "metadata": {},
   "source": [
    "The other inputs to `sgd_poly` are the initial guess `theta0`$=\\boldsymbol\\theta_0$, the initial learning rate `t0`$=t_0$, which defines the learning rate $t_k = t_0/(k+1)$ in the $k$-th iteration, and the fixed number of iterations `iters`. The function `sgd_poly` should compute and return both `theta` **and** the loss `L(theta, V, Y)` as defined in Task 1 of Problem Sheet 7 at the final iteration.\n",
    "\n",
    "_Hint: you may find the_ [np.random.randint](https://numpy.org/doc/stable/reference/random/generated/numpy.random.randint.html) _function useful._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b89bacc",
   "metadata": {},
   "source": [
    "## Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08360d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L(theta, V, Y):\n",
    "    return np.linalg.norm(V @ theta - Y)**2 / Y.size\n",
    "\n",
    "def sgd_poly(V, Y, theta0, t0, iters=10000):\n",
    "    theta = theta0\n",
    "    for k in range(iters):\n",
    "        i = np.random.randint(low=0, high=V.shape[0])\n",
    "        vk = 2 * V[i].T * (V[i] @ theta - Y[i])   # Note the (...) is a number\n",
    "        tk = t0/(k+1)\n",
    "        theta = theta - tk * vk    \n",
    "    return theta, L(theta, V, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caf23d1",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "- Write a Python code that applies `sgd_poly` to compute $\\boldsymbol\\theta_k \\approx \\boldsymbol\\theta^* = \\arg\\min L(\\boldsymbol\\theta)$ for $L(\\boldsymbol\\theta)$ defined in Task 1, starting from $\\boldsymbol\\theta_0 = \\mathbf{0}$, using default `iters`, and $t_0$ which you should select from the range $1/4,1/2,1,2,4$ such that the algorithm produces the smallest loss on average over a few (up to 10) restarts. What is this $t_0$ and the corresponding loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685a2ced",
   "metadata": {},
   "source": [
    "## Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98b5582c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t0 = 0.25, avg. final loss = 3.168e-02\n",
      "t0 = 0.5, avg. final loss = 2.089e-02\n",
      "t0 = 1, avg. final loss = 1.533e-02\n",
      "t0 = 2, avg. final loss = 1.135e-02\n",
      "t0 = 4, avg. final loss = 1.393e+00\n"
     ]
    }
   ],
   "source": [
    "nrestarts = 10\n",
    "for t0 in [1/4,1/2,1,2,4]:\n",
    "    Loss = np.zeros(nrestarts)\n",
    "    for i in range(nrestarts):\n",
    "        theta, Loss[i] = sgd_poly(V, Y, np.zeros(n+1), t0)\n",
    "    print(f\"t0 = %g, avg. final loss = %3.3e\" % (t0, np.mean(Loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f4aee2",
   "metadata": {},
   "source": [
    "$t_0=1$ or $2$ depending on the particular realisation of data. Increasing `Nsamples` and `nrestarts` can give a more reproducible estimate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
